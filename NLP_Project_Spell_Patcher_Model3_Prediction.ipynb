{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "0f4c3e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from collections import namedtuple\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "import time\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "47a81da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_book(path):\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file) as f:\n",
    "        book = f.read()\n",
    "    return book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "7dfd5183",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './books/'\n",
    "book_files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "book_files = book_files[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "48d5c55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = []\n",
    "for book in book_files:\n",
    "    books.append(load_book(path+book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "f5ff2cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 126999 words in Pride_and_Prejudice_by_Jane_Austen.rtf.\n",
      "There are 113452 words in David_Copperfield_by_Charles_Dickens.rtf.\n",
      "There are 194282 words in The_Romance_of_Lust_by_Anonymous.rtf.\n",
      "There are 25395 words in Metamorphosis_by_Franz_Kafka.rtf.\n",
      "There are 191598 words in Great_Expectations_by_Charles_Dickens.rtf.\n",
      "There are 165188 words in Oliver_Twist_by_Charles_Dickens.rtf.\n",
      "There are 53211 words in The_Prince_by_Nicolo_Machiavelli.rtf.\n",
      "There are 96185 words in The_Adventures_of_Tom_Sawyer_by_Mark_Twain.rtf.\n",
      "There are 480495 words in The_Count_of_Monte_Cristo_by_Alexandre_Dumas.rtf.\n",
      "There are 78912 words in Frankenstein_by_Mary_Shelley.rtf.\n",
      "There are 33464 words in Through_the_Looking_Glass_by_Lewis_Carroll.rtf.\n",
      "There are 9463 words in The_Yellow_Wallpaper_by_Charlotte_Perkins_Gilman.rtf.\n",
      "There are 166996 words in Dracula_by_Bram_Stoker.rtf.\n",
      "There are 163109 words in Emma_by_Jane_Austen.rtf.\n",
      "There are 105428 words in Grimms_Fairy_Tales_by_The_Brothers_Grimm.rtf.\n",
      "There are 83657 words in The_Picture_of_Dorian_Gray_by_Oscar_Wilde.rtf.\n",
      "There are 30423 words in Alices_Adventures_in_Wonderland_by_Lewis_Carroll.rtf.\n",
      "There are 433993 words in Don_Quixote_by_Miguel_de_Cervantes.rtf.\n",
      "There are 361612 words in Anna_Karenina_by_Leo_Tolstoy.rtf.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(books)):\n",
    "    print(\"There are {} words in {}.\".format(len(books[i].split()), book_files[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "8bc81472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'\\n', ' ', text) \n",
    "    text = re.sub(r'[{}@_*>()\\\\#%+=\\[\\]]','', text)\n",
    "    text = re.sub('a0','', text)\n",
    "    text = re.sub('\\'92t','\\'t', text)\n",
    "    text = re.sub('\\'92s','\\'s', text)\n",
    "    text = re.sub('\\'92m','\\'m', text)\n",
    "    text = re.sub('\\'92ll','\\'ll', text)\n",
    "    text = re.sub('\\'91','', text)\n",
    "    text = re.sub('\\'92','', text)\n",
    "    text = re.sub('\\'93','', text)\n",
    "    text = re.sub('\\'94','', text)\n",
    "    text = re.sub('\\.','. ', text)\n",
    "    text = re.sub('\\!','! ', text)\n",
    "    text = re.sub('\\?','? ', text)\n",
    "    text = re.sub(' +',' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "38795a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_books = []\n",
    "for book in books:\n",
    "    clean_books.append(clean_text(book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "9d8a2b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int = {}\n",
    "count = 0\n",
    "for book in clean_books:\n",
    "    for character in book:\n",
    "        if character not in vocab_to_int:\n",
    "            vocab_to_int[character] = count\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "459e485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = ['<PAD>','<EOS>','<GO>']\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = count\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "fa537a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary contains 78 characters.\n",
      "[' ', '!', '\"', '$', '&', \"'\", ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<EOS>', '<GO>', '<PAD>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab_to_int)\n",
    "print(\"The vocabulary contains {} characters.\".format(vocab_size))\n",
    "print(sorted(vocab_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "26cdbad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_vocab = {}\n",
    "for character, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "c5a99342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 127068 sentences.\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for book in clean_books:\n",
    "    for sentence in book.split('. '):\n",
    "        sentences.append(sentence + '.')\n",
    "print(\"There are {} sentences.\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "ac8a7f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    int_sentence = []\n",
    "    for character in sentence:\n",
    "        int_sentence.append(vocab_to_int[character])\n",
    "    int_sentences.append(int_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "afde386d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98648 sentences available to train and test our model.\n"
     ]
    }
   ],
   "source": [
    "max_length = 200\n",
    "min_length = 10\n",
    "\n",
    "good_sentences = []\n",
    "\n",
    "for sentence in int_sentences:\n",
    "    if len(sentence) <= max_length and len(sentence) >= min_length:\n",
    "        good_sentences.append(sentence)\n",
    "        \n",
    "\n",
    "print(\"{} sentences available to train and test our model.\".format(len(good_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "4f4b8b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 73986\n",
      "Number of testing and validation sentences: 24662\n",
      "Number of validation sentences: 14797\n",
      "Number of testing sentences: 9865\n"
     ]
    }
   ],
   "source": [
    "training, remaining = train_test_split(good_sentences, test_size = 0.25, random_state = 2)\n",
    "\n",
    "print(\"Number of training sentences:\", len(training))\n",
    "print(\"Number of testing and validation sentences:\", len(remaining))\n",
    "\n",
    "validation, testing = train_test_split(remaining, test_size = 0.4, random_state = 2)\n",
    "print(\"Number of validation sentences:\", len(validation))\n",
    "print(\"Number of testing sentences:\", len(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "bcf83027",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sorted = []\n",
    "validation_sorted = []\n",
    "testing_sorted = []\n",
    "\n",
    "\n",
    "for i in range(min_length, max_length+1):\n",
    "    for sentence in training:\n",
    "        if len(sentence) == i:\n",
    "            training_sorted.append(sentence)\n",
    "    for sentence in validation:\n",
    "        if len(sentence) == i:\n",
    "            validation_sorted.append(sentence)\n",
    "    for sentence in testing:\n",
    "        if len(sentence) == i:\n",
    "            testing_sorted.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "eedbe104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 56, 41, 33, 32, 36, 50, 19, 60, 42] 10\n",
      "[37, 4, 9, 1, 7, 6, 1, 7, 5, 42] 10\n",
      "[55, 23, 8, 1, 7, 13, 5, 19, 49, 42] 10\n",
      "[63, 7, 8, 30, 2, 7, 23, 20, 22, 42] 10\n",
      "[55, 23, 8, 1, 7, 13, 5, 19, 49, 42] 10\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(training_sorted[i], len(training_sorted[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "282e25c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
    "           'n','o','p','q','r','s','t','u','v','w','x','y','z',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "6adf51c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_maker(sentence, threshold):\n",
    "    \n",
    "    noisy_sentence = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        random = np.random.uniform(0,1,1)\n",
    "        if random < threshold:\n",
    "            noisy_sentence.append(sentence[i])\n",
    "        else:\n",
    "            new_random = np.random.uniform(0,1,1)\n",
    "            if new_random > 0.67:\n",
    "                if i == (len(sentence) - 1):\n",
    "                    continue\n",
    "                else:\n",
    "                    noisy_sentence.append(sentence[i+1])\n",
    "                    noisy_sentence.append(sentence[i])\n",
    "                    i += 1\n",
    "            elif new_random < 0.33:\n",
    "                random_letter = np.random.choice(letters, 1)[0]\n",
    "                noisy_sentence.append(vocab_to_int[random_letter])\n",
    "                noisy_sentence.append(sentence[i])\n",
    "            else:\n",
    "                pass     \n",
    "        i += 1\n",
    "    return noisy_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "d2546677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 56, 41, 33, 32, 36, 50, 19, 60, 42]\n",
      "[25, 56, 41, 32, 36, 50, 19, 60, 42]\n",
      "\n",
      "[37, 4, 9, 1, 7, 6, 1, 7, 5, 42]\n",
      "[37, 4, 9, 1, 7, 6, 1, 7, 5, 42]\n",
      "\n",
      "[55, 23, 8, 1, 7, 13, 5, 19, 49, 42]\n",
      "[55, 23, 8, 1, 7, 13, 5, 19, 49, 42]\n",
      "\n",
      "[63, 7, 8, 30, 2, 7, 23, 20, 22, 42]\n",
      "[63, 7, 8, 30, 2, 7, 23, 20, 22, 42]\n",
      "\n",
      "[55, 23, 8, 1, 7, 13, 5, 19, 49, 42]\n",
      "[55, 23, 8, 16, 1, 7, 13, 5, 19, 49, 42]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.9\n",
    "for sentence in training_sorted[:5]:\n",
    "    print(sentence)\n",
    "    print(noise_maker(sentence, threshold))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "af403967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    \n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    with tf.name_scope('targets'):\n",
    "        targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    inputs_length = tf.placeholder(tf.int32, (None,), name='inputs_length')\n",
    "    targets_length = tf.placeholder(tf.int32, (None,), name='targets_length')\n",
    "    max_target_length = tf.reduce_max(targets_length, name='max_target_len')\n",
    "\n",
    "    return inputs, targets, keep_prob, inputs_length, targets_length, max_target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "692f7517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(targets, vocab_to_int, batch_size):\n",
    "    \n",
    "    with tf.name_scope(\"process_encoding\"):\n",
    "        ending = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])\n",
    "        dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "183e785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob, direction):\n",
    "    \n",
    "    if direction == 1:\n",
    "        with tf.name_scope(\"RNN_Encoder_Cell_1D\"):\n",
    "            for layer in range(num_layers):\n",
    "                with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "                    lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "\n",
    "                    drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "\n",
    "                    enc_output, enc_state = tf.nn.dynamic_rnn(drop,rnn_inputs,sequence_length,dtype=tf.float32)\n",
    "\n",
    "            return enc_output, enc_state\n",
    "        \n",
    "        \n",
    "    if direction == 2:\n",
    "        with tf.name_scope(\"RNN_Encoder_Cell_2D\"):\n",
    "            for layer in range(num_layers):\n",
    "                with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "                    cell_fw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "                    cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw,input_keep_prob = keep_prob)\n",
    "\n",
    "                    cell_bw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "                    cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw,input_keep_prob = keep_prob)\n",
    "\n",
    "                    enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw,cell_bw,rnn_inputs,sequence_length,dtype=tf.float32)\n",
    "\n",
    "\n",
    "            enc_output = tf.concat(enc_output,2)\n",
    "            return enc_output, enc_state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "22d13dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_decoding_layer(dec_embed_input, targets_length, dec_cell, initial_state, output_layer,vocab_size, max_target_length):\n",
    "    \n",
    "    with tf.name_scope(\"Training_Decoder\"):\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,sequence_length=targets_length,time_major=False)\n",
    "\n",
    "\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,training_helper,initial_state,output_layer)\n",
    "\n",
    "\n",
    "        training_logits, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,output_time_major=False,impute_finished=True,maximum_iterations=max_target_length)\n",
    "\n",
    "\n",
    "        return training_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "c1b681ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,max_target_length, batch_size):\n",
    "    \n",
    "    with tf.name_scope(\"Inference_Decoder\"):\n",
    "        start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,start_tokens,end_token)\n",
    "\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,inference_helper,initial_state,output_layer)\n",
    "\n",
    "        inference_logits, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,output_time_major=False,impute_finished=True,maximum_iterations=max_target_length)\n",
    "\n",
    "        return inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "7e045c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, \n",
    "                   max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers, direction):\n",
    "    \n",
    "    with tf.name_scope(\"RNN_Decoder_Cell\"):\n",
    "        for layer in range(num_layers):\n",
    "            with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "                lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "                dec_cell = tf.contrib.rnn.DropoutWrapper(lstm,input_keep_prob = keep_prob)\n",
    "    \n",
    "    output_layer = Dense(vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,enc_output,inputs_length,normalize=False,name='BahdanauAttention')\n",
    "    \n",
    "    with tf.name_scope(\"Attention_Wrapper\"):\n",
    "        dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,attn_mech,rnn_size)\n",
    "    \n",
    "    initial_state =  dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state)\n",
    "\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,sequence_length=targets_length,time_major=False)\n",
    "\n",
    "\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,training_helper,initial_state,output_layer) \n",
    "\n",
    "\n",
    "        training_logits, _ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,output_time_major=False,impute_finished=True,maximum_iterations=max_target_length)\n",
    "        \n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "\n",
    "        start_tokens = tf.tile(tf.constant([vocab_to_int['<GO>']], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "        end_token = (tf.constant(vocab_to_int['<EOS>'], dtype=tf.int32))\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,start_tokens,end_token)\n",
    "\n",
    "\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,inference_helper,initial_state,output_layer)\n",
    "\n",
    "\n",
    "        inference_logits, _ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,output_time_major=False,impute_finished=True,maximum_iterations=max_target_length)\n",
    "\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "42a8b679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(inputs, targets, keep_prob, inputs_length, targets_length, max_target_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size, embedding_size, direction):\n",
    "    \n",
    "    enc_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
    "    enc_embed_input = tf.nn.embedding_lookup(enc_embeddings, inputs)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, inputs_length, num_layers, enc_embed_input, keep_prob, direction)\n",
    "    \n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
    "    dec_input = process_encoding_input(targets, vocab_to_int, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, dec_embeddings,enc_output,enc_state, vocab_size, inputs_length,targets_length, \n",
    "                                                        max_target_length,rnn_size, vocab_to_int, keep_prob, batch_size,num_layers,direction)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "cb67b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "44d53c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(sentences, batch_size, threshold):\n",
    "    \n",
    "    for batch_i in range(0, len(sentences)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        sentences_batch = sentences[start_i:start_i + batch_size]\n",
    "        \n",
    "        sentences_batch_noisy = []\n",
    "        for sentence in sentences_batch:\n",
    "            sentences_batch_noisy.append(noise_maker(sentence, threshold))\n",
    "            \n",
    "        sentences_batch_eos = []\n",
    "        for sentence in sentences_batch:\n",
    "            sentence.append(vocab_to_int['<EOS>'])\n",
    "            sentences_batch_eos.append(sentence)\n",
    "            \n",
    "        pad_sentences_batch = np.array(pad_sentence_batch(sentences_batch_eos))\n",
    "        pad_sentences_noisy_batch = np.array(pad_sentence_batch(sentences_batch_noisy))\n",
    "        \n",
    "        pad_sentences_lengths = []\n",
    "        for sentence in pad_sentences_batch:\n",
    "            pad_sentences_lengths.append(len(sentence))\n",
    "        \n",
    "        pad_sentences_noisy_lengths = []\n",
    "        for sentence in pad_sentences_noisy_batch:\n",
    "            pad_sentences_noisy_lengths.append(len(sentence))\n",
    "        \n",
    "        yield pad_sentences_noisy_batch, pad_sentences_batch, pad_sentences_noisy_lengths, pad_sentences_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "6016e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "num_layers = 2\n",
    "rnn_size = 512\n",
    "embedding_size = 128\n",
    "learning_rate = 0.0005\n",
    "direction = 2\n",
    "threshold = 0.99\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "da44b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(keep_prob, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    inputs, targets, keep_prob, inputs_length, targets_length, max_target_length = model_inputs()\n",
    "\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(inputs, [-1]),targets,keep_prob,inputs_length,targets_length,max_target_length,len(vocab_to_int)+1,\n",
    "                                                      rnn_size,num_layers,vocab_to_int,batch_size,embedding_size,direction)\n",
    "\n",
    "\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "\n",
    "    with tf.name_scope('predictions'):\n",
    "        predictions = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "        tf.summary.histogram('predictions', predictions)\n",
    "\n",
    "    masks = tf.sequence_mask(targets_length, max_target_length, dtype=tf.float32, name='masks')\n",
    "    \n",
    "    with tf.name_scope(\"cost\"):\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(training_logits, \n",
    "                                                targets, \n",
    "                                                masks)\n",
    "        tf.summary.scalar('cost', cost)\n",
    "\n",
    "    with tf.name_scope(\"optimze\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "\n",
    "    merged = tf.summary.merge_all()    \n",
    "\n",
    "    export_nodes = ['inputs', 'targets', 'keep_prob', 'cost', 'inputs_length', 'targets_length',\n",
    "                    'predictions', 'merged', 'train_op','optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "3c7b79c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, log_string):\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # wandb.tensorflow.log(tf.summary.merge_all())\n",
    "\n",
    "        testing_loss_summary = []\n",
    "\n",
    "        iteration = 0\n",
    "        \n",
    "        display_step = 30 \n",
    "        stop_early = 0 \n",
    "        stop = 5 \n",
    "        per_epoch = 3 \n",
    "        testing_check = (len(training_sorted)//batch_size//per_epoch)-1\n",
    "\n",
    "        print()\n",
    "        print(\"Training Model: {}\".format(log_string))\n",
    "\n",
    "        train_writer = tf.summary.FileWriter('./logs/1/train/{}'.format(log_string), sess.graph)\n",
    "        test_writer = tf.summary.FileWriter('./logs/1/test/{}'.format(log_string))\n",
    "\n",
    "        for epoch_i in range(1, epochs+1): \n",
    "            batch_loss = 0\n",
    "            batch_time = 0\n",
    "            \n",
    "            for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(\n",
    "                    get_batches(training_sorted, batch_size, threshold)):\n",
    "                start_time = time.time()\n",
    "\n",
    "                summary, loss, _ = sess.run([model.merged,\n",
    "                                             model.cost, \n",
    "                                             model.train_op], \n",
    "                                             {model.inputs: input_batch,\n",
    "                                              model.targets: target_batch,\n",
    "                                              model.inputs_length: input_length,\n",
    "                                              model.targets_length: target_length,\n",
    "                                              model.keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "                batch_loss += loss\n",
    "                wandb.log({\"Training_Loss\": loss})\n",
    "                end_time = time.time()\n",
    "                batch_time += end_time - start_time\n",
    "\n",
    "                train_writer.add_summary(summary, iteration)\n",
    "\n",
    "                iteration += 1\n",
    "\n",
    "                if batch_i % display_step == 0 and batch_i > 0:\n",
    "                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                          .format(epoch_i,\n",
    "                                  epochs, \n",
    "                                  batch_i, \n",
    "                                  len(training_sorted) // batch_size, \n",
    "                                  batch_loss / display_step, \n",
    "                                  batch_time))\n",
    "                    batch_loss = 0\n",
    "                    batch_time = 0\n",
    "\n",
    "                if batch_i % testing_check == 0 and batch_i > 0:\n",
    "                    batch_loss_testing = 0\n",
    "                    batch_time_testing = 0\n",
    "                    for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(\n",
    "                            get_batches(validation_sorted, batch_size, threshold)):\n",
    "                        start_time_testing = time.time()\n",
    "                        summary, loss = sess.run([model.merged,\n",
    "                                                  model.cost], \n",
    "                                                     {model.inputs: input_batch,\n",
    "                                                      model.targets: target_batch,\n",
    "                                                      model.inputs_length: input_length,\n",
    "                                                      model.targets_length: target_length,\n",
    "                                                      model.keep_prob: 1})\n",
    "\n",
    "                        batch_loss_testing += loss\n",
    "                        wandb.log({\"Testing_Loss\":loss})\n",
    "                        end_time_testing = time.time()\n",
    "                        batch_time_testing += end_time_testing - start_time_testing\n",
    "\n",
    "                        test_writer.add_summary(summary, iteration)\n",
    "\n",
    "                    n_batches_testing = batch_i + 1\n",
    "                    print('Testing Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                          .format(batch_loss_testing / n_batches_testing, \n",
    "                                  batch_time_testing))\n",
    "                    \n",
    "                    batch_time_testing = 0\n",
    "\n",
    "                    testing_loss_summary.append(batch_loss_testing)\n",
    "                    if batch_loss_testing <= min(testing_loss_summary):\n",
    "                        print('New Record!') \n",
    "                        stop_early = 0\n",
    "                        checkpoint = \"./model/{}.ckpt\".format(log_string)\n",
    "                        saver = tf.train.Saver()\n",
    "                        saver.save(sess, checkpoint)\n",
    "\n",
    "                    else:\n",
    "                        print(\"No Improvement.\")\n",
    "                        stop_early += 1\n",
    "                        if stop_early == stop:\n",
    "                            break\n",
    "\n",
    "            if stop_early == stop:\n",
    "                print(\"Stopping Training.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "e4efee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_sentences(sentence_id_list):\n",
    "    resulting_sentence_list = []\n",
    "    pad = vocab_to_int[\"<PAD>\"] \n",
    "    for sentence in sentence_id_list:\n",
    "        resulting_sentence_list.append(\"\".join([int_to_vocab[i] for i in sentence if i != pad]))\n",
    "\n",
    "    return resulting_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "47aaf715",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"./model3/kp=0.75,nl=2,th=0.95.ckpt\"\n",
    "path_to_ckpt_meta = \"./model3/kp=0.75,nl=2,th=0.95.ckpt.meta\"\n",
    "path_to_ckpt_data = \"./model3/kp=0.75,nl=2,th=0.95.ckpt.data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "a35e6042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_ints(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    return [vocab_to_int[word] for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "b159d75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd620ecb710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd620ecb710>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd620ec0650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd620ec0650>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd61f851fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd61f851fd0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd61f851410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd61f851410>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd61fb3de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd61fb3de10>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fd623683350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fd623683350>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd622fdbc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd622fdbc50>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd61f851e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd61f851e90>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd623485210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd623485210>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd61f675310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd61f675310>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fd623683350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fd623683350>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd622fdbc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd622fdbc50>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd61f851e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd61f851e90>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd623485210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd623485210>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd61f675310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd61f675310>>: AttributeError: module 'gast' has no attribute 'Index'\n"
     ]
    }
   ],
   "source": [
    "model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction) \n",
    "\n",
    "pad = vocab_to_int[\"<PAD>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "9e5213aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTION FOR USER INPUT\n",
    "\n",
    "def predict(text):\n",
    "    text = text_to_ints(text)\n",
    "    with tf.Session() as sess:\n",
    "        # Load saved model\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, checkpoint)\n",
    "\n",
    "        answer_logits = sess.run(model.predictions, {model.inputs: [text]*batch_size, \n",
    "                                                     model.inputs_length: [len(text)]*batch_size,\n",
    "                                                     model.targets_length: [len(text)+1], \n",
    "                                                     model.keep_prob: [1.0]})[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print('\\nText')\n",
    "        print('  Word Ids:    {}'.format([i for i in text]))\n",
    "        print('  Input Words: {}'.format(\"\".join([int_to_vocab[i] for i in text])))\n",
    "\n",
    "        print('\\nSummary')\n",
    "        print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "        print('  Response Words: {}'.format(\"\".join([int_to_vocab[i] for i in answer_logits if i != pad])))\n",
    "        print(\"-------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "c2282672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text\n",
      "  Word Ids:    [32, 24, 23, 19, 6, 8, 24, 13, 13, 13, 20, 19, 17, 23, 20, 20, 19, 0, 4, 5, 10, 19, 5, 28]\n",
      "  Input Words: The schoool bell rang nw\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [32, 24, 23, 19, 6, 8, 24, 13, 13, 20, 19, 17, 23, 20, 20, 19, 0, 4, 5, 10, 19, 5, 13, 28, 19]\n",
      "  Response Words: The school bell rang now \n",
      "-------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Deletion and insertion\n",
    "predict(\"The schoool bell rang nw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "5a3c5357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text\n",
      "  Word Ids:    [32, 24, 23, 19, 5, 23, 28, 19, 8, 16, 9, 8, 4, 30, 23, 6, 19, 4, 0, 23, 19, 5, 13, 28, 28, 19, 6, 23, 20, 20, 7, 5]\n",
      "  Input Words: The new cupcakes are noww sellin\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [32, 24, 23, 19, 5, 23, 28, 19, 8, 16, 9, 8, 4, 30, 23, 6, 19, 4, 0, 23, 19, 5, 13, 28, 19, 6, 23, 20, 20, 7, 5, 10, 19]\n",
      "  Response Words: The new cupcakes are now selling \n",
      "-------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Deletion and insertion\n",
    "predict(\"The new cupcakes are noww sellin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "370810c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text\n",
      "  Word Ids:    [23, 56, 19, 7, 6, 19, 28, 0, 7, 1, 5, 7, 10, 19, 4, 19, 20, 23, 1, 1, 23, 0]\n",
      "  Input Words: eH is writnig a letter\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [56, 23, 19, 7, 6, 19, 28, 0, 7, 1, 7, 5, 10, 19, 4, 19, 20, 23, 1, 1, 23, 0, 19]\n",
      "  Response Words: He is writing a letter \n",
      "-------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Two swaps\n",
    "predict(\"eH is writnig a letter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "75534e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text\n",
      "  Word Ids:    [32, 24, 23, 19, 2, 7, 6, 24, 19, 6, 28, 4, 21, 19, 7, 5, 19, 1, 24, 23, 19, 6, 23]\n",
      "  Input Words: The fish swam in the se\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [32, 24, 23, 19, 2, 7, 6, 24, 19, 6, 28, 4, 21, 19, 7, 5, 19, 1, 24, 23, 19, 6, 23, 4]\n",
      "  Response Words: The fish swam in the sea\n",
      "-------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Contextual insertion\n",
    "predict(\"The fish swam in the se\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "f29dddc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text\n",
      "  Word Ids:    [61, 0, 6, 42, 19, 37, 23, 5, 5, 23, 1, 19, 28, 4, 5, 1, 23, 22, 19, 1, 13, 19, 6, 23, 19, 24, 23, 0, 19, 21, 13, 1, 24, 23, 0]\n",
      "  Input Words: Mrs. Bennet wanted to se her mother\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [61, 0, 6, 19, 37, 23, 5, 5, 23, 1, 19, 28, 4, 5, 1, 23, 22, 19, 1, 13, 19, 6, 23, 23, 19, 24, 23, 0, 19, 21, 13, 1, 24, 23, 0, 19]\n",
      "  Response Words: Mrs Bennet wanted to see her mother \n",
      "-------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Contextual Insertion\n",
    "predict(\"Mrs. Bennet wanted to se her mother\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "a3505bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text\n",
      "  Word Ids:    [55, 24, 23, 19, 1, 13, 30, 13, 19, 4, 19, 34, 4, 0, 19, 2, 0, 21, 13, 19, 1, 24, 23, 19, 6, 24, 23, 20, 2]\n",
      "  Input Words: She toko a jar frmo the shelf\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [55, 24, 23, 19, 1, 13, 13, 30, 19, 4, 19, 34, 4, 0, 19, 2, 0, 13, 21, 19, 1, 24, 23, 19, 6, 24, 23, 20, 2, 19]\n",
      "  Response Words: She took a jar from the shelf \n",
      "-------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Two swaps\n",
    "predict(\"She toko a jar frmo the shelf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "8ab50fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text\n",
      "  Word Ids:    [56, 7, 6, 19, 6, 7, 6, 6, 1, 23, 0, 19, 7, 6, 19, 4, 19, 6, 7, 5, 10, 23]\n",
      "  Input Words: His sisster is a singe\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [56, 7, 6, 19, 6, 7, 6, 1, 23, 0, 19, 7, 6, 19, 4, 19, 6, 7, 5, 10, 23, 0, 23]\n",
      "  Response Words: His sister is a singere\n",
      "-------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mistaken insertion \n",
    "predict(\"His sisster is a singe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "a39b940a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text\n",
      "  Word Ids:    [56, 23, 19, 7, 6, 19, 6, 23, 20, 20, 7, 5, 19, 24, 7, 6, 19, 24, 13, 16, 6, 23]\n",
      "  Input Words: He is sellin his house\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [56, 23, 19, 7, 6, 19, 6, 23, 20, 20, 7, 5, 19, 24, 7, 6, 19, 24, 13, 16, 6, 23, 19]\n",
      "  Response Words: He is sellin his house \n",
      "-------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Undetected error\n",
    "predict(\"He is sellin his house\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "22a260d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL EVALUATION ON THE TEST SET\n",
    "\n",
    "def evaluate(testing_sorted):\n",
    "\n",
    "    # Check to ensure noise_maker is making mistakes correctly.\n",
    "    noisy_sentence_list = []\n",
    "    threshold = 0.99\n",
    "    for sentence in testing_sorted:\n",
    "        noisy_sentence_list.append(noise_maker(sentence, threshold))\n",
    "\n",
    "        text_list = noisy_sentence_list\n",
    "\n",
    "    # for text in text_list:\n",
    "    #     text = text_to_ints(text)\n",
    "\n",
    "    predictions_list = []\n",
    "\n",
    "    # random = np.random.randint(0,len(testing_sorted))\n",
    "    # text = testing_sorted[random]\n",
    "    # text = noise_maker(text, 0.95)\n",
    "\n",
    "    checkpoint = \"./model3/kp=0.75,nl=2,th=0.95.ckpt\"\n",
    "\n",
    "    prediction_list = []\n",
    "    with tf.Session() as sess:\n",
    "        # Load saved model\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, checkpoint)\n",
    "\n",
    "        for text in text_list:\n",
    "\n",
    "            answer_logits = sess.run(model.predictions, {model.inputs: [text]*batch_size, \n",
    "                                                         model.inputs_length: [len(text)]*batch_size,\n",
    "                                                         model.targets_length: [len(text)+1], \n",
    "                                                         model.keep_prob: [1.0]})[0]\n",
    "\n",
    "            prediction_list.append(\"\".join([int_to_vocab[i] for i in answer_logits if i != pad]))\n",
    "\n",
    "    return prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "3f68cca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = ids_to_sentences(testing_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "4eccc083",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = evaluate(testing_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "d350904d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.825686517089071e-232\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "reference_list = list(map(lambda x: x.split(\" \"),reference))\n",
    "hypothesis_list = list(map(lambda x: x.split(\" \"),hypothesis))\n",
    "\n",
    "print(corpus_bleu(reference_list,hypothesis_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e87c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
